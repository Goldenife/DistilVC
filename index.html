<!DOCTYPE html>
<!-- saved from url=(0033)https://QicongXie.github.io/end2endvc/ -->
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Begin Jekyll SEO tag v2.7.1 -->
  <title>https://github.com/Goldenife/SynthVC: Leveraging Synthetic Data for End-to-End Low Latency Streaming Voice Conversion</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="TODO: title">
  <meta property="og:locale" content="en_US">
  <meta name="twitter:card" content="summary">
  <!-- End Jekyll SEO tag -->

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
  <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->
  </section>

  <section class="main-content">
    <h1 id="">
      <center>https://github.com/Goldenife/SynthVC: Leveraging Synthetic Data for End-to-End Low Latency Streaming Voice Conversion</center>
    </h1>

    <!-- <center>Ziqian Ning<sup>1, 2</sup>, Shuai Wang<sup>3</sup>, Pengcheng Zhu<sup>2</sup>, Zhichao Wang<sup>1</sup>, Jixun Yao<sup>1</sup>, Lei Xie<sup>1</sup>, Mengxiao Bi<sup>2</sup></center> -->
    <!-- <center><a href="http://www.npu-aslp.org"><sup>1</sup>Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science,</br>Northwestern Polytechnical University, Xi'an, China </a></center> -->
    <!-- <center><a href="https://fuxi.163.com/laboratory"><sup>2</sup>Fuxi AI Lab, NetEase Inc., Hangzhou, China </a></center> -->
    <!-- <center><a href="http://www.sribd.cn/en"><sup>3</sup>Shenzhen Research Institute of Big Data,</br>The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China</a></center> -->

    <br><br>
    <h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
    <p>Voice Conversion (VC) modifies a speaker's timbre while preserving linguistic content. While showing effectiveness in speaker similarity and robustness, most existing VC models face challenges in real-time scenarios which require low-latency streaming capabilities. Streaming VC models typically use automatic speech recognition (ASR) models to extract linguistic content or rely on speaker representation disentanglement (SRD) techniques. However, these methods often suffer from timbre leakage, high computational latency, and compromised speech naturalness. To address these limitations, we present https://github.com/Goldenife/SynthVC, a streaming end-to-end voice conversion framework that establishes direct timbre mapping between source and target speakers. Our method eliminates the reliance on complex disentanglement design through the utilization of synthetic parallel training data. Experimental results demonstrate that https://github.com/Goldenife/SynthVC achieves low latency and outperforms existing systems in both naturalness and speaker similarity. To facilitate reproducibility and community advancement, we will release the complete implementation code and pre-trained models upon publication.</p>

    <table frame=void rules=none>
      <tr>
        <center><img src='raw/fig/https://github.com/Goldenife/SynthVC_v4.jpg'></center>
      </tr>
      <tr>
      </tr>
    </table>
    <br><br>

    <!-- <h2>2. Computational Metrics</h2>
    <ul>
      <table>
  <tbody>
    <tr>
      <td></td>
      <td>RTF</td>
      <td>Latency (ms)</td>
      <td>Params (M)</td>
    </tr>
    <tr>
      <td>Full mode</td>
      <td>0.797</td>
      <td>15.94+20+20=55.94</td>
      <td>22.7</td>
    </tr>
    <tr>
      <td>&emsp;AM (w/ 2 pseudo ctx)</td>
      <td>0.201</td>
      <td>4.02</td>
      <td>10.9</td>
    </tr>
    <tr>
      <td>&emsp;Vocoder (w/ 2 pseudo ctx)</td>
      <td>0.086</td>
      <td>1.72</td>
      <td>1.2</td>
    </tr>
    <tr>
      <td>&emsp;LM</td>
      <td>0.510</td>
      <td>10.20</td>
      <td>10.6</td>
    </tr>
    <tr>
      <td>Stand-alone mode</td>
      <td>0.181</td>
      <td>3.58+20+20=43.58</td>
      <td>12.1</td>
    </tr>
    <tr>
      <td>&emsp;AM (w/o pseudo ctx)</td>
      <td>0.134</td>
      <td>2.68</td>
      <td>10.9</td>
    </tr>
    <tr>
      <td>&emsp;Vocoder (w/o pseudo ctx)</td>
      <td>0.047</td>
      <td>0.90</td>
      <td>1.2</td>
    </tr>
  </tbody>
  <colgroup>
    <col>
    <col>
    <col>
    <col>
  </colgroup>
</table>
    </ul> -->

<!--     <h2>3. Demo<a name="Comparison"></a></h2>
    <ul>
      <li>DualVC2: Streaming mode of DualVC 2 [1].</li>
      <li>VQMIVC: Non-streaming mode of VQMIVC [2].</li>
      <li>VQMIVC-streaming: Streaming mode of VQMIVC.</li>
      <li>DualVC3-full: Full mode of DualVC 3.</li>
      <li>DualVC3-standalone: Standalone mode of DualVC 3.</li>
    </ul>

    <table>
      <tbody id="tbody">
      </tbody>
    </table> 

    </br>
    <cite>[1] Z. Ning, Y. Jiang, P. Zhu, S. Wang, J. Yao, L. Xie, and M. Bi, “Dualvc 2: Dynamic masked convolution for unified streaming and non-streaming voice conversion,” in Proc. ICASSP. IEEE, 2024, pp. 1–5.</cite>
    </br>
    <cite>[2] D. Wang, L. Deng, Y. T. Yeung, X. Chen, X. Liu, and H. Meng, “VQMIVC: vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion,” in Proc. INTERSPEECH. ISCA, 2021, pp. 1344–1348.</cite>
  </section>
</body> -->

<!-- <script type="text/javascript">
  window.onload = function () {
    let scenes = ["Clean"]
    let speakers = ["female", "male"]
    let genders = ["female", "male"]
    let models = ["DualVC2", "VQMIVC", "VQMIVC-streaming", "DualVC3-full", "DualVC3-standalone"]
    let all_samples = [["SSB00430070.wav", "SSB02460443.wav", "SSB03230482.wav", "SSB03410353.wav", "SSB03750317.wav", "SSB04700097.wav", "SSB05900051.wav", "SSB07100255.wav", "SSB07100388.wav", "SSB08170086.wav",], 
                    ["SSB00430070.wav", "SSB02460443.wav", "SSB03230482.wav", "SSB03410353.wav", "SSB03750317.wav", "SSB04700097.wav", "SSB05900051.wav", "SSB07100255.wav", "SSB07100388.wav", "SSB08170086.wav",]]
    let sample_data = `
        <tr>
          <td style="text-align: center; width: 150px;" rowspan=2><strong>Target Speaker<strong></td>
          <td style="text-align: center; width: 150px;" rowspan=2><strong>Source speech<strong></td>
          <td style="text-align: center; width: 150px;" colspan=5><strong>Method<strong></td>
        </tr>
